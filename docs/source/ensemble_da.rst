Ensemble-Based Data Assimilation
================================

Overview
--------

The ensemble data assimilation problem is defined in this section. Consider
a 1D function :math:`f_{t}(x) \in V`, where :math:`V` is a function space, at time
:math:`t` that either has a random initial condition, :math:`f_{0} \sim \eta`,
or a random component, such as a stochastic forcing term. An ensemble of these
functions, :math:`\Big\{f^{i}_{t}\Big\}_{i=1,...,N}`, can be generated by sampling i.i.d
:math:`f_{0}^{i} \sim \eta`. E.g.

.. code::
    
    ensemble = []
    for i in range(N):
        f = Function(V).assign(numpy.random.normal(0, 1))
        ensemble.append(f)

Once these so-called particle functions are generated, the ensemble represents
a particle approximation to the distribution :math:`p(f_{t})` via

.. math:: p(f_{t}) \approx \frac{1}{N} \sum_{i=1}^{N} \delta(f_{t} - f_{t}^{i}),

where :math:`\delta` is a delta function. Consider a reference function :math:`r_{t} \in V`,
that one knows to be the truth,

.. math:: r_{t}(x) = \sum_{l=1}N_{l}(x)\xi_{l}.

where :math:`N_{l}(x)` are the standard basis functions of :math:`V`. Observations
are taken from it via

.. math:: y^{j}_{t} = r_{t}^{per}(x^{j}),

with :math:`r_{t}^{per}` given by perturbing the basis coefficients by a random variable,

.. math:: r_{t}^{per}(x) = \sum_{l=1}N_{l}(x)(\xi_{l} + \phi_{l}),

:math:`j=1,...,n_{y}`, where :math:`\phi_{l} \sim N(0, R)` and :math:`x^{j}` are
coordinates that the reference function is evaluated at. Denote the list of observations
at time :math:`t` to be :math:`Y_{t}`.

.. code::

    observations = Observations(V, R)

Using standard importance sampling, one can
compute normalized importance weight functions :math:`w_{t}^{i} \in V` for each
function in the ensemble. These lead to the particle approximation to the distribution
:math:`p(f_{t}|Y_{t})` via

.. math:: p(f_{t}|Y_{t}) \approx \sum^{N}_{i=1} w^{i}_{t} \delta(f_{t} - f_{t}^{i}).


Sequential Importance Sampling
------------------------------

Sequential importance sampling is the recursive version of the above particle approximation.
If we work with the sequence of time-steps where observations become availbale, known as
assimilation steps, :math:`t_{k}`, :math:`k=1,2,3,...`, and set the initial weights of each
function to take the value :math:`1/N` everywhere then one can represent the following
Bayesian update

.. math:: p(f_{t_{k}}|Y_{t_{1}},...,Y_{t_{k}}) \propto p(f_{t_{k}}|Y_{t_{1}},...,Y_{t_{k-1}})p(Y_{t_{k}}|f_{t_{k}})

by

.. math:: w_{t_{k}}^{i} \propto w_{t_{k-1}}^{i}p(Y_{t_{k}}|f_{t_{k}}^{i}).

Once normalized, the above weight functions satisfy the new particle approximation for
the distribution :math:`p(f_{t_{k}}|Y_{t_{1}},...,Y_{t_{k}})`

.. math:: p(f_{t_{k}}|Y_{t_{1}},...,Y_{t_{k}}) \approx \sum_{i=1}^{N}w_{t_{k}}^{i}\delta(f_{t_{k}} - f_{t_{k}}^{i}).

To compute one step of the sequential importance sampling weight update in FADE, given a list of
coordinates and a corresponding list of observations, the following commands can be used:

.. code::
    
    weights = []
    for i in range(N):
        w = Function(V).assign(1.0 / N)
        weights.append(w)
    
    observation_operator.update_observation_operator(coordinates,
                                                     observations)
    
    weights = weight_update(weights, ensemble, observation_operator)


Ensemble-Transform Particle Filtering
-------------------------------------
