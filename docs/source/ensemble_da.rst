Ensemble-Based Data Assimilation
================================

Overview
--------

Data assimilation is the incorporation of data into a forecast model of an uncertain process.
It is used frequently in many fields ranging from finance to weather forecasting. Filtering is
a type of data assimilation, where one is interested in the forecast distribution
of an uncertain temporal (and spatial) process at a certain time in the future conditional on
observations of the actual event given up until that time. This Python package allows one to apply
a number of filtering / forecast verification techniques on functions that are finite-element
discretized solutions to PDEs formulated in the novel Firedrake software, developed at Imperial
College London.

The ensemble filtering (particle filtering) problem is defined in this section. Consider
a 1D function :math:`f_{t}(x) \in V`, where :math:`V` is a function space, at time
:math:`t` that either has a random initial condition, :math:`f_{0} \sim \eta`,
or a random component, such as a stochastic forcing term. An ensemble of these
functions, :math:`\Big\{f^{i}_{t}\Big\}_{i=1,...,N}`, can be generated by sampling i.i.d
:math:`f_{0}^{i} \sim \eta`. E.g.

.. code::
    
    # define the ensemble, each function being randomly assigned a value
    ensemble = []
    for i in range(N):
        f = Function(V).assign(numpy.random.normal(0, 1))
        ensemble.append(f)

Once these so-called particle functions are generated, the ensemble represents
a particle approximation to the distribution :math:`p(f_{t})` via

.. math:: p(f_{t}) \approx \frac{1}{N} \sum_{i=1}^{N} \delta(f_{t} - f_{t}^{i}),

where :math:`\delta` is a delta function. Consider a reference function :math:`r_{t} \in V`,
that one knows to be the truth,

.. math:: r_{t}(x) = \sum_{l=1}N_{l}(x)\xi_{l}.

where :math:`N_{l}(x)` are the standard basis functions of :math:`V`. Observations
are taken from it via

.. math:: y^{j}_{t} = r_{t}^{per}(x^{j}),

with :math:`r_{t}^{per}` given by perturbing the basis coefficients by a random variable,

.. math:: r_{t}^{per}(x) = \sum_{l=1}N_{l}(x)(\xi_{l} + \phi_{l}),

:math:`j=1,...,n_{y}`, where :math:`\phi_{l} \sim N(0, R)` and :math:`x^{j}` are
coordinates that the reference function is evaluated at. Note: this does mean that all basis
coefficients are assumed to have independent measurement error in the reference function.
Denote the list of observations at time :math:`t` to be :math:`Y_{t}`. In FADE, one needs to initialize an
object that carries out the projection of observations into ensemble space to compute likelihoods. This
is done via the following command:

.. code::

    observation_operator = Observations(V, R)

Using standard importance sampling, one can
compute normalized importance weight functions :math:`w_{t}^{i} \in V` for each
function in the ensemble. These lead to the particle approximation to the distribution
:math:`p(f_{t}|Y_{t})` via

.. math:: p(f_{t}|Y_{t}) \approx \sum^{N}_{i=1} w^{i}_{t} \delta(f_{t} - f_{t}^{i}).


Sequential Importance Sampling
------------------------------

Sequential importance sampling is the recursive version of the above particle approximation.
If we work with the sequence of time-steps where observations become available, known as
assimilation steps, :math:`t_{k}`, :math:`k=1,2,3,...`, and set the initial weights of each
function to take the value :math:`1/N` everywhere then one can represent the following
Bayesian update

.. math:: p(f_{t_{k}}|Y_{t_{1}},...,Y_{t_{k}}) \propto p(f_{t_{k}}|Y_{t_{1}},...,Y_{t_{k-1}})p(Y_{t_{k}}|f_{t_{k}})

by

.. math:: w_{t_{k}}^{i} \propto w_{t_{k-1}}^{i}p(Y_{t_{k}}|f_{t_{k}}^{i}).

Once normalized, the above weight functions satisfy the new particle approximation for
the distribution :math:`p(f_{t_{k}}|Y_{t_{1}},...,Y_{t_{k}})`

.. math:: p(f_{t_{k}}|Y_{t_{1}},...,Y_{t_{k}}) \approx \sum_{i=1}^{N}w_{t_{k}}^{i}\delta(f_{t_{k}} - f_{t_{k}}^{i}).

To compute one step of the sequential importance sampling weight update in FADE, given a list of
`coordinates` and a corresponding list of `observations`, the following commands can be used:

.. code::
    
    # define initial weights
    weights = []
    for i in range(N):
        w = Function(V).assign(1.0 / N)
        weights.append(w)
    
    # update observation operator with observations
    observation_operator.update_observation_operator(coordinates,
                                                     observations)
    # update the importance weights
    weights = weight_update(ensemble, weights, observation_operator)

Localisation can be used here, by adding an optional argument to the method above. For a full
summary of how this implemented in FADE see :ref:`localisation`.

Ensemble-Transform Particle Filtering
-------------------------------------

Using the data assimilation process above for a large number of assimilation steps can cause
degeneracy amongst weights; where one normalized weight function tends towards one everywhere
and every other one takes very small values. Many particle filtering algorithms counter this using
resampling. The aim is to generate a new ensemble of functions :math:`\Big\{\tilde{f}^{i}_{t}\Big\}_{i=1,...,N}` with even weights, from the weighted ensemble found using an importance weight update
above. This transform can be a random resampling process or a deterministic transformation; either
way it is desirable to have

.. math:: \frac{1}{N}\sum_{i=1}^{N}\tilde{f}_{t_{k}}^{i} \approx \sum_{i=1}^{N}w_{t_{k}}^{i}f_{t_{k}}^{i}.

The main difference between variants of the particle filter is the way in which that step is carried
out. In the variants that are used in this package, the Ensemble Transform Particle Filter (Reich,
2011) and the corresponding multilevel Monte Carlo extension (Gregory et al, 2016), a deterministic
transform is implemented. This actually makes the two terms in the approximation above equal. One can
carry out this transform using:

.. code::
    
    ensemble = ensemble_transform_update(ensemble, weights)

Localisation can be used as in the weight update case, this again being specified using an optional
argument.
